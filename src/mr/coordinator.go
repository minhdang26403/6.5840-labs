package mr

import (
	"fmt"
	"log"
	"net"
	"net/http"
	"net/rpc"
	"os"
	"sync"
	"time"
)

// State of workers
const (
	idle = iota
	running
	completed
)

// Phase of the MapReduce computation
const (
	Map = iota
	Reduce
)

type Coordinator struct {
	files   []string   // input files to process
	nMap    int        // number of input splits
	nReduce int        // number of `Reduce` partitions
	mu      sync.Mutex // lock to protect the internal state of the coordinator
	phase   int        // the phase of MapReduce (0 is Map phase, 1 is Reduce phase)
	ifiles  [][]string // files generated by Map tasks

	numMapGiven   int // the number of Map tasks assigned
	numMapRunning int // the number of Map tasks running
	mapPhase      sync.Cond
	mapTaskState  []int

	numReduceGiven   int // the number of Reduce tasks assigned
	numReduceRunning int // the number of Reduce tasks running
	reducePhase      sync.Cond
	reduceTaskState  []int
}

// RPC handlers for the worker to call.

// A worker calls `GetMapTask` to get a new Map task to process
func (c *Coordinator) GetMapTask(args *MapArgs, reply *MapReply) error {
	c.mu.Lock()
	defer c.mu.Unlock()
	taskNumber := c.numMapGiven
	found := false
	if c.numMapGiven == c.nMap {
		for !found {
			if c.numMapRunning == 0 {
				return fmt.Errorf("Map phase done")
			}
			c.mapPhase.Wait()
			if c.numMapGiven == c.nMap && c.numMapRunning == 0 {
				return fmt.Errorf("Map phase done")
			}
			for i := 0; i < c.nMap; i++ {
				if c.mapTaskState[i] == idle {
					taskNumber = i
					found = true
					break
				}
			}
		}
	}

	reply.Filename = c.files[taskNumber]
	reply.TaskNumber = taskNumber
	reply.NReduce = c.nReduce
	c.mapTaskState[taskNumber] = running
	c.numMapGiven++
	c.numMapRunning++
	return nil
}

// A worker calls `CompleteMapTask` to signal the coordinator that
// it has completed a Map task
func (c *Coordinator) CompleteMapTask(args *MapArgs, reply *MapReply) error {
	c.mu.Lock()
	defer c.mu.Unlock()
	if c.mapTaskState[args.TaskNumber] == completed {
		return fmt.Errorf("Message from a completed Map task")
	}
	c.numMapRunning--
	c.mapTaskState[args.TaskNumber] = completed
	for bucket, filename := range args.IntermediateFiles {
		c.ifiles[bucket] = append(c.ifiles[bucket], filename)
	}
	if c.numMapGiven == c.nMap && c.numMapRunning == 0 {
		c.mapPhase.Broadcast()
		c.phase = Reduce
	}
	return nil
}

// A worker calls `GetReduceTask` to get a new Reduce task to process
func (c *Coordinator) GetReduceTask(args *ReduceArgs, reply *ReduceReply) error {
	c.mu.Lock()
	defer c.mu.Unlock()
	taskNumber := c.numReduceGiven
	found := false
	if c.numReduceGiven == c.nReduce {
		for !found {
			if c.numReduceRunning == 0 {
				return fmt.Errorf("Reduce phase done")
			}
			c.reducePhase.Wait()
			if c.numReduceGiven == c.nReduce && c.numReduceRunning == 0 {
				return fmt.Errorf("Reduce phase done")
			}
			for i := 0; i < c.nMap; i++ {
				if c.reduceTaskState[i] == idle {
					taskNumber = i
					found = true
					break
				}
			}
		}
	}

	reply.TaskNumber = taskNumber
	reply.IntermediateFiles = c.ifiles[taskNumber]
	c.reduceTaskState[taskNumber] = running
	c.numReduceGiven++
	c.numReduceRunning++

	return nil
}

// A worker calls `CompleteReduceTask` to signal the coordinator that
// it has completed a Reduce task
func (c *Coordinator) CompleteReduceTask(args *ReduceArgs, reply *ReduceReply) error {
	c.mu.Lock()
	defer c.mu.Unlock()
	if c.reduceTaskState[args.TaskNumber] == completed {
		return fmt.Errorf("Message from a completed Reduce task")
	}
	c.numReduceRunning--
	c.reduceTaskState[args.TaskNumber] = completed
	if c.numReduceGiven == c.nReduce && c.numReduceRunning == 0 {
		c.reducePhase.Broadcast()
	}

	return nil
}

// start a thread that listens for RPCs from worker.go
func (c *Coordinator) server() {
	rpc.Register(c)
	rpc.HandleHTTP()
	//l, e := net.Listen("tcp", ":1234")
	sockname := coordinatorSock()
	os.Remove(sockname)
	l, e := net.Listen("unix", sockname)
	if e != nil {
		log.Fatal("listen error:", e)
	}
	go http.Serve(l, nil)
}

// main/mrcoordinator.go calls Done() periodically to find out
// if the entire job has finished.
func (c *Coordinator) Done() bool {
	ret := false

	c.mu.Lock()
	defer c.mu.Unlock()
	if c.numReduceGiven == c.nReduce && c.numReduceRunning == 0 {
		ret = true
	}

	return ret
}

func (c *Coordinator) CheckCrashedWorkers() {
	c.mu.Lock()
	for c.numMapGiven != c.nMap {
		c.mu.Unlock()
		time.Sleep(5 * time.Second)
		c.mu.Lock()
	}

	// c.mu.Lock()
	for c.phase == Map {
		c.mu.Unlock()
		time.Sleep(10 * time.Second)
		c.mu.Lock()
		if c.phase != 0 {
			break
		}
		for i := 0; i < c.nMap; i++ {
			if c.mapTaskState[i] == running {
				c.numMapGiven--
				c.numMapRunning--
				c.mapTaskState[i] = idle
				c.mapPhase.Signal()
			}
		}
	}
	c.mu.Unlock()

	for !c.Done() {
		time.Sleep(10 * time.Second)
		if c.Done() {
			break
		}
		c.mu.Lock()
		for i := 0; i < c.nReduce; i++ {
			if c.reduceTaskState[i] == running {
				c.numReduceGiven--
				c.numReduceRunning--
				c.reduceTaskState[i] = idle
				c.reducePhase.Signal()
			}
		}
		c.mu.Unlock()
	}
}

// create a Coordinator.
// main/mrcoordinator.go calls this function.
// nReduce is the number of reduce tasks to use.
func MakeCoordinator(files []string, nReduce int) *Coordinator {
	c := Coordinator{
		files:           files,
		nMap:            len(files),
		nReduce:         nReduce,
		phase:           Map,
		ifiles:          make([][]string, nReduce),
		mapTaskState:    make([]int, len(files)),
		reduceTaskState: make([]int, nReduce),
	}
	c.mapPhase = *sync.NewCond(&c.mu)
	c.reducePhase = *sync.NewCond(&c.mu)

	// Start a background thread that periodically checks slow or failed workers
	go c.CheckCrashedWorkers()

	c.server()
	return &c
}
